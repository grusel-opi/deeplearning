{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character level language model with RNNs\n",
    "\n",
    "In the last exercise, we implemented MLP and CNNs in PyTorch on MNIST and CIFAR datasets. In this exercise, we will implement a character level language model with RNNs and train it on Shakespearean text.\n",
    "\n",
    "This code is based on Andrej Karpathy's [char-rnn](https://github.com/karpathy/char-rnn). Do check out his [blog](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [gist](https://gist.github.com/karpathy/d4dee566867f8291f086) about the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Shakespearean text is provided in the ```ex_05_nlp_graph.zip``` under ```./data/tinyshakespeare.txt```. We will load the data and perform some preprocessing for starters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the contents of the text file\n",
    "data = open('./data/tinyshakespeare.txt', 'r').read()\n",
    "print (data[:1000]) # let's examine some text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text looks good but it can not be processed by RNNs in its raw form. We first need to tokenize the data and convert it into a form suitable for RNNs. Since we focus on character level language models in this exercise, we will consider each character as a token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = list(set(data)) # get the set of unique characters\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print ('The text file has {} characters out of which {} are unique.'.format(data_size, vocab_size))\n",
    "print (chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have quite a big text with 65 unique characters. Now, we need to associate each character with a unique id which can then be converted into 1-hot vector form to provide as input to the RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creat a dictionary mapping each character to a unique id and vice versa\n",
    "char_to_ix = {ch:i for i,ch in enumerate(chars)}\n",
    "ix_to_char = {i:ch for i,ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a unique id for each character, we can represent each character with a 1-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-hot encoding example for a few tokens\n",
    "for _ in range(5):\n",
    "    random_char_index = np.random.randint(0, vocab_size)\n",
    "    random_char = ix_to_char[random_char_index]\n",
    "    one_hot_vector = np.zeros(vocab_size)\n",
    "    one_hot_vector[random_char_index] = 1\n",
    "    print (random_char_index, random_char, one_hot_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last exercise on classification with MNIST and CIFAR, we had ground truth labels provided explicitly with each instance of the dataset. In our text dataset, we don't have explicitly ground truth labels but note that in a character level language model, we predict the next character. So, in essence, our text is itself the ground truth label since for each character, the next character acts as the ground truth. This will be important when loading the data for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have preprocessed the data, we are ready to start building and training our model. But, recall that in the last exercise, we use MNIST and CIFAR datasets which are available in the torchvision package, which we directly supplied to PyTorch's DataLoader class. However, PyTorch's DataLoader takes in a specific object of the inbuilt [Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) class. So, we first need to convert our dataset in this form by inheriting from the Dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General template for defining a CustomDataset by inheriting from the Dataset class\n",
    "# We need to override the __init__, __len__ and __getitem__ methods\n",
    "# __init__ is called during the dataset instantiation\n",
    "# __len__ returns the size of the dataset\n",
    "# __getitem__ is called during training and returns the data to be used during training\n",
    "# Refer to the Dataset class link for more information\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        super(CustomDataset, self).__init__()\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        pass\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first task of this exercise is to implement the ```__init__```, ```__len__``` and ```__getitem__``` methods for our dataset. The preprocessing steps that we did above are now to be implemented as methods of our dataset class. We have implemented the ```__init__``` and ```__len__``` methods for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "# TO-DO: Implement the __getitem__ of the Shakespeare class. \n",
    "# Important points: __getitem__ is called at each training iteration\n",
    "# So, we need to return the data and ground-truth label. The data is in\n",
    "# the form of one-hot vectors and ground-truth is the index of next char\n",
    "# Our RNN operates on an input sequence of a specified length (seq_length)\n",
    "# so we need to return a sequence of one-hot vector and the indices of\n",
    "# their corresponding next character\n",
    "#########################################################################\n",
    "\n",
    "class Shakespeare(Dataset):\n",
    "    \n",
    "    def __init__(self, data_path, seq_length):\n",
    "        super(Shakespeare, self).__init__()\n",
    "        \n",
    "        self.seq_length = seq_length # length of the input sequence\n",
    "        \n",
    "        # same as done above\n",
    "        self.data = open(data_path, 'r').read()\n",
    "        self.data_size = len(self.data)\n",
    "        \n",
    "        self.chars = list(set(self.data))\n",
    "        self.vocab_size = len(self.chars)\n",
    "        data_size, vocab_size = len(self.data), len(self.chars)\n",
    "        \n",
    "        self.char_to_ix = {ch:i for i,ch in enumerate(self.chars)}\n",
    "        self.ix_to_char = {i:ch for i,ch in enumerate(self.chars)}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data_size - self.seq_length - 1\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        ##### implement this part #####\n",
    "        \n",
    "        \n",
    "        ###############################\n",
    "        \n",
    "        # one_hot_input_seq = one-hot vectors of the tokens in input sequence\n",
    "        # targets = indices of the next character of each token\n",
    "        \n",
    "        # one_hot_input_seq = (seq_length, vocab_size)\n",
    "        # targets = (seq_length)\n",
    "        return one_hot_input_seq, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined our dataset, we can instatiate it and build our dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 25\n",
    "batch_size = 100\n",
    "\n",
    "data_path = './data/tinyshakespeare.txt'\n",
    "dataset = Shakespeare(data_path, seq_length)\n",
    "vocab_size = dataset.vocab_size\n",
    "\n",
    "# Q: why is drop_last=True and shuffle=True used in the dataloader?\n",
    "# What happens if we don't set them to True?\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=1, shuffle=True, drop_last=True)\n",
    "\n",
    "\n",
    "#########################################################################\n",
    "# Solution\n",
    "# drop_last=True is used since the data size might not be divisible by batch_size, \n",
    "# so the last batch would have less elements and cannot be processed by the network\n",
    "#\n",
    "# shuffle=False leads to sequential data loading which enables propagating hidden state\n",
    "# from one batch to the next, this helps the network to learn better since it provides\n",
    "# relevant context and the network doesn't need to start from 0 everytime\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=1, shuffle=False, drop_last=True)\n",
    "#########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: why is drop_last=True and shuffle=True used in the dataloader? What happens if we don't set them to True?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to define our RNN model before training. For this, we will implement the RNN Cell discussed in lecture 8 slide 6. Note that the RNN Cell operates on a single timestep. So, the RNN Cell will take a single timestep token as input and produce output and hidden state for that particular timestep only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "# TO-DO: Implement the __init__ and forward methods of the RNNCell class. \n",
    "# Refer to the equations in the lecture and implement the same here\n",
    "# The forward method should return the output and the hidden state\n",
    "#########################################################################\n",
    "\n",
    "class RNNCell(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super(RNNCell, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        ##### implement this part #####\n",
    "\n",
    "        \n",
    "        ###############################\n",
    "        \n",
    "    def forward(self, input_emb, hidden_state):\n",
    "        \n",
    "        # input_emb = (batch_size, vocab_size)\n",
    "        # hidden_state = (batch_size, hidden_size)\n",
    "        \n",
    "        ##### implement this part #####\n",
    "        \n",
    "        \n",
    "        ###############################\n",
    "        \n",
    "        return output, hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have a sequence of tokens as input, we will implement another model which uses this RNNCell and processes multi-timestep sequence inputs. The RNN class takes in a sequence of one-hot encodings of tokens as input and returns a sequence of output, one for each timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "# TO-DO: Implement the forward method of the RNN class. \n",
    "# The RNN class takes in a sequence of one-hot encodings of tokens as input \n",
    "# and returns a sequence of output, one for each timestep.\n",
    "# We also return the hidden state of the final timestep\n",
    "# Q: Is it required to return the hidden state? If yes, why? If no, why?\n",
    "#########################################################################\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, seq_length, vocab_size, hidden_size):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.seq_length = seq_length\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn_cell = RNNCell(vocab_size, hidden_size)\n",
    "        \n",
    "    def forward(self, input_seq, hidden_state):\n",
    "        \n",
    "        # input_seq: (batch_size, seq_length, vocab_size)\n",
    "        # hidden_state: (batch_size, hidden_size)\n",
    "        \n",
    "        ##### implement this part #####\n",
    "        \n",
    "        \n",
    "        ###############################\n",
    "        \n",
    "        # outputs: (batch_size, seq_lenth, vocab_size)\n",
    "        # hidden_state: (batch_size, hidden_size)\n",
    "        \n",
    "        return outputs, hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that dataset and model definitions are done, we need to implement the training loop and we are good to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "# TO-DO: Implement the missing part of the training function. \n",
    "# As a loss function we want to use cross-entropy\n",
    "# It can be called with F.cross_entropy().\n",
    "# Hint: Pass through the model -> Backpropagate gradients -> Take gradient step\n",
    "#########################################################################\n",
    "\n",
    "def train(model, dataloader, optimizer, epoch, log_interval, device='cpu'):\n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        # we get these from __getitem__ of the dataset class that we implemented earlier\n",
    "        # data: (batch_size, seq_length, vocab_size)\n",
    "        # target: (batch_size, seq_length)\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # first we need to zero the gradient, otherwise PyTorch would accumulate them\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Q: Is hidden_state required to be initialized to 0 here?\n",
    "        # If yes, why? If no, why? How does this affect the training?\n",
    "        hidden_state = torch.zeros(batch_size, hidden_size).to(device)\n",
    "        \n",
    "        ##### implement this part #####\n",
    "        \n",
    "        \n",
    "        ###############################\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(dataloader.dataset),\n",
    "                100. * batch_idx / len(dataloader), loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we instantiate our model and optimizer and then we can start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model and training parameters\n",
    "# feel free to experiment with different parameters and optimizers\n",
    "hidden_size = 100\n",
    "learning_rate = 3e-2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "rnn = RNN(seq_length, vocab_size, hidden_size).to(device)\n",
    "\n",
    "optimizer = optim.Adagrad(rnn.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "epochs = 3\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(rnn, dataloader, optimizer, epoch, log_interval=1000, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Along with training the model, it's also good to check what kind of text our model is generating. We have implemented a function which samples text from the model give an initial token and a hidden state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample text of length seq_len, this seq_len need to be the same\n",
    "# as seq_length that we used earlier, we can basically sample text\n",
    "# of any arbitrary length.\n",
    "\n",
    "def sample(hidden_state, token, seq_len):\n",
    "    token_emb = torch.zeros(1, vocab_size).to(device) # use batch_size=1 at inference\n",
    "    token_emb[0,token] = 1\n",
    "    char_indices = [token] # first token\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for timestep in range(seq_len):\n",
    "            output, hidden_state = rnn.rnn_cell(token_emb, hidden_state)\n",
    "            output = torch.softmax(output, dim=-1) # convert to probabilities\n",
    "            token = torch.argmax(output, dim=-1).item() # get the token with the highest proability\n",
    "            char_indices.append(token)\n",
    "\n",
    "            token_emb = torch.zeros(1, vocab_size).to(device)\n",
    "            token_emb[0,token] = 1\n",
    "    \n",
    "    return char_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's sample sample text from the model after every epoch to see if our model is learning to generate some text or not. In the code below, we are sampling a 100 char text from the model, starting with a random token and 0 memory. Try to generate some text by using the the hidden_state returned by RNN class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(rnn, dataloader, optimizer, epoch, log_interval=1000, device=device)\n",
    "    \n",
    "    # sample a 100 char text from the model, starting with a random token and 0 memory\n",
    "    token = np.random.randint(0, vocab_size)\n",
    "    hidden_state = torch.zeros(1, hidden_size).to(device)\n",
    "    char_indices = sample(hidden_state, token, 100) # sample a 100 char text from the model\n",
    "    txt = ''.join(dataset.ix_to_char[ix] for ix in char_indices)\n",
    "    print (txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything went well, then our model should be able to generate some legible text after some epochs. However, it'd probably be quite slow at it. Try to figure out how to make the model learn faster. Use the code from [here](https://gist.github.com/karpathy/d4dee566867f8291f086) as reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: It has to do with initializing hidden state to 0 in every training iteration in the training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model should be able to learn spellings and certain words, use of spaces and how to begin sentences. It most likely won't be able to generate long sentences. To generate long sentences and passages, the model capacity needs to be increased. Try using a 3-layer RNN with 512 dimensional hidden state and see if the model is able to generate better text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus: Try implementing a word level language model with the same model on the same dataset. The only difference is that now each word is a token rather than each character. So, the tokenization in the dataset needs to be changed and everything else remains same"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
