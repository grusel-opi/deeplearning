{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "\n",
    "In the last exercises, we took care of the dataloading and mini-batching for you. With PyTorch, this task is considerably easier, so let's do it ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gandalf/miniconda3/envs/deep_learning_ex_1/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/gandalf/miniconda3/envs/deep_learning_ex_1/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.3%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "4.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# torchvision contains convinience functions for popular datasets\n",
    "ds_train = datasets.MNIST('data', train=True, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<PIL.Image.Image image mode=L size=28x28 at 0x7FB16D057370>, 5)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=120x120>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if we index this dataset, we get a single data point: a PIL image and an Integer\n",
    "print(ds_train[0])\n",
    "ds_train[0][0].resize((120,120))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's transform the data to something that our Pytorch models will understand\n",
    "for this purpose, we can supply a transform function to the datase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize((0.0), (1.0))\n",
    "])\n",
    "ds_train = datasets.MNIST('data', train=True, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image is now a torch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ds_train[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normalization is something you learned about in the lecture. Normalizing with $\\mu=0, \\sigma=1$ corresponds to no normalization. Let's compute the proper normalization constants!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets get only the images \n",
    "ims_train = ds_train.data\n",
    "ims_train = ims_train.float() / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "(std, mu) = torch.std_mean(ims_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mu, std)\n",
    "])\n",
    "ds_train = datasets.MNIST('data', train=True, download=True, transform=transform)\n",
    "ds_test = datasets.MNIST('data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to receive mini-batches, not only single data points.\n",
    "We use PyTorch's DataLoader class. Build a dataloader with a batch size of 64 and 2 workers (number of subprocess that peform the dataloading. Important: you need to shuffle the training data, not the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = DataLoader(dataset=ds_train, batch_size=64, shuffle=True, num_workers=2)\n",
    "dl_test = DataLoader(dataset=ds_test, batch_size=64, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mT_co\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msampler\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbatch_sampler\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSampler\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnum_workers\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcollate_fn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdrop_last\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mworker_init_fn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmultiprocessing_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mprefetch_factor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpersistent_workers\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpin_memory_device\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Data loader. Combines a dataset and a sampler, and provides an iterable over\n",
      "the given dataset.\n",
      "\n",
      "The :class:`~torch.utils.data.DataLoader` supports both map-style and\n",
      "iterable-style datasets with single- or multi-process loading, customizing\n",
      "loading order and optional automatic batching (collation) and memory pinning.\n",
      "\n",
      "See :py:mod:`torch.utils.data` documentation page for more details.\n",
      "\n",
      "Args:\n",
      "    dataset (Dataset): dataset from which to load the data.\n",
      "    batch_size (int, optional): how many samples per batch to load\n",
      "        (default: ``1``).\n",
      "    shuffle (bool, optional): set to ``True`` to have the data reshuffled\n",
      "        at every epoch (default: ``False``).\n",
      "    sampler (Sampler or Iterable, optional): defines the strategy to draw\n",
      "        samples from the dataset. Can be any ``Iterable`` with ``__len__``\n",
      "        implemented. If specified, :attr:`shuffle` must not be specified.\n",
      "    batch_sampler (Sampler or Iterable, optional): like :attr:`sampler`, but\n",
      "        returns a batch of indices at a time. Mutually exclusive with\n",
      "        :attr:`batch_size`, :attr:`shuffle`, :attr:`sampler`,\n",
      "        and :attr:`drop_last`.\n",
      "    num_workers (int, optional): how many subprocesses to use for data\n",
      "        loading. ``0`` means that the data will be loaded in the main process.\n",
      "        (default: ``0``)\n",
      "    collate_fn (Callable, optional): merges a list of samples to form a\n",
      "        mini-batch of Tensor(s).  Used when using batched loading from a\n",
      "        map-style dataset.\n",
      "    pin_memory (bool, optional): If ``True``, the data loader will copy Tensors\n",
      "        into device/CUDA pinned memory before returning them.  If your data elements\n",
      "        are a custom type, or your :attr:`collate_fn` returns a batch that is a custom type,\n",
      "        see the example below.\n",
      "    drop_last (bool, optional): set to ``True`` to drop the last incomplete batch,\n",
      "        if the dataset size is not divisible by the batch size. If ``False`` and\n",
      "        the size of dataset is not divisible by the batch size, then the last batch\n",
      "        will be smaller. (default: ``False``)\n",
      "    timeout (numeric, optional): if positive, the timeout value for collecting a batch\n",
      "        from workers. Should always be non-negative. (default: ``0``)\n",
      "    worker_init_fn (Callable, optional): If not ``None``, this will be called on each\n",
      "        worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as\n",
      "        input, after seeding and before data loading. (default: ``None``)\n",
      "    generator (torch.Generator, optional): If not ``None``, this RNG will be used\n",
      "        by RandomSampler to generate random indexes and multiprocessing to generate\n",
      "        `base_seed` for workers. (default: ``None``)\n",
      "    prefetch_factor (int, optional, keyword-only arg): Number of batches loaded\n",
      "        in advance by each worker. ``2`` means there will be a total of\n",
      "        2 * num_workers batches prefetched across all workers. (default value depends\n",
      "        on the set value for num_workers. If value of num_workers=0 default is ``None``.\n",
      "        Otherwise if value of num_workers>0 default is ``2``).\n",
      "    persistent_workers (bool, optional): If ``True``, the data loader will not shutdown\n",
      "        the worker processes after a dataset has been consumed once. This allows to\n",
      "        maintain the workers `Dataset` instances alive. (default: ``False``)\n",
      "    pin_memory_device (str, optional): the data loader will copy Tensors\n",
      "        into device pinned memory before returning them if pin_memory is set to true.\n",
      "\n",
      "\n",
      ".. warning:: If the ``spawn`` start method is used, :attr:`worker_init_fn`\n",
      "             cannot be an unpicklable object, e.g., a lambda function. See\n",
      "             :ref:`multiprocessing-best-practices` on more details related\n",
      "             to multiprocessing in PyTorch.\n",
      "\n",
      ".. warning:: ``len(dataloader)`` heuristic is based on the length of the sampler used.\n",
      "             When :attr:`dataset` is an :class:`~torch.utils.data.IterableDataset`,\n",
      "             it instead returns an estimate based on ``len(dataset) / batch_size``, with proper\n",
      "             rounding depending on :attr:`drop_last`, regardless of multi-process loading\n",
      "             configurations. This represents the best guess PyTorch can make because PyTorch\n",
      "             trusts user :attr:`dataset` code in correctly handling multi-process\n",
      "             loading to avoid duplicate data.\n",
      "\n",
      "             However, if sharding results in multiple workers having incomplete last batches,\n",
      "             this estimate can still be inaccurate, because (1) an otherwise complete batch can\n",
      "             be broken into multiple ones and (2) more than one batch worth of samples can be\n",
      "             dropped when :attr:`drop_last` is set. Unfortunately, PyTorch can not detect such\n",
      "             cases in general.\n",
      "\n",
      "             See `Dataset Types`_ for more details on these two types of datasets and how\n",
      "             :class:`~torch.utils.data.IterableDataset` interacts with\n",
      "             `Multi-process data loading`_.\n",
      "\n",
      ".. warning:: See :ref:`reproducibility`, and :ref:`dataloader-workers-random-seed`, and\n",
      "             :ref:`data-loading-randomness` notes for random seed related questions.\n",
      "\u001b[0;31mFile:\u001b[0m           ~/miniconda3/envs/deep_learning_ex_1/lib/python3.10/site-packages/torch/utils/data/dataloader.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     \n"
     ]
    }
   ],
   "source": [
    "?DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP in Pytorch\n",
    "\n",
    "Ok, the dataloading works. Let's build our model, PyTorch makes this very easy. We will build replicate the model from our last exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the parameters to be used\n",
    "nInput = 784\n",
    "nOutput = 10\n",
    "nLayer = 1\n",
    "nHidden = 64\n",
    "act_fn = nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "# TODO: Implement the __init__ of the MLP class. \n",
    "# insert the activation after every linear layer. Important: the number of \n",
    "# hidden layers should be variable!\n",
    "#########################################################################\n",
    "\n",
    "def fc_layer(c_in, c_out, act_func):\n",
    "    return nn.Sequential(nn.Linear(c_in, c_out), act_fn)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, nInput, nOutput, nLayer, nHidden, act_fn):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = [] \n",
    "        \n",
    "        layers += fc_layer(nInput, nHidden, nn.ReLU())\n",
    "\n",
    "        for _ in range(nLayer):\n",
    "            layers += fc_layer(nHidden, nHidden, nn.ReLU())\n",
    "    \n",
    "        layers += fc_layer(nHidden, nOutput, nn.ReLU())\n",
    "                            \n",
    "        layers.append([nn.LogSoftmax(dim=1)]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's test if the forward pass works\n",
    "# this should print torch.Size([1, 10])\n",
    "t = torch.randn(1,1,28,28)\n",
    "mlp = MLP(nInput, nOutput, nLayer, nHidden, act_fn)\n",
    "mlp(t).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already implemented the test function for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dl_test, device='cpu'):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in dl_test:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(dl_test.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)\\n'.format(\n",
    "        test_loss, correct, len(dl_test.dataset),\n",
    "        100. * correct / len(dl_test.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we only need to implement the training and we are good to go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dl_train, optimizer, epoch, log=True, log_interval=100, device='cpu'):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(dl_train):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # first we need to zero the gradient, otherwise PyTorch would accumulate them\n",
    "        optimizer.zero_grad()         \n",
    "        \n",
    "        ##### implement this part #####\n",
    "        \n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target=target)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        ###############################\n",
    "\n",
    "        # stats\n",
    "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        if log and batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\r'.format(\n",
    "                epoch, batch_idx * len(data), len(dl_train.dataset),\n",
    "                100. * batch_idx / len(dl_train), loss.item()))\n",
    "    if log:\n",
    "        print('\\nTrain set: Average loss: {:.4f}, Accuracy: {}/{} ({:.1f}%)\\n'.format(\n",
    "            loss, correct, len(dl_train.dataset),\n",
    "            100. * correct / len(dl_train.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??torch.mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, the setup is almost done. The onoly missing part is the optimizer. We are going to use AdaDelta. You already saw AdaGrad in the last exercise, AdaDelta is a version of AdaGrad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reinitialize the mlp, so we can play with parameters right here\n",
    "mlp = MLP(nInput, nOutput, 0, nHidden, act_fn)\n",
    "learning_rate = 1.0\n",
    "optimizer = optim.Adadelta(mlp.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.002571\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.047185\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.003617\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.011074\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.000772\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.002557\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.000632\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.007219\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.045224\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.070567\n",
      "\n",
      "Train set: Average loss: 0.0342, Accuracy: 59555/60000 (99.3%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1104, Accuracy: 9745/10000 (97.450%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.001161\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.001470\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.004568\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.084617\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.181823\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.003945\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.000339\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.041371\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.046477\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.034229\n",
      "\n",
      "Train set: Average loss: 0.0021, Accuracy: 59596/60000 (99.3%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1302, Accuracy: 9718/10000 (97.180%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.008907\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.001377\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.016145\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.003116\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.015429\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.010989\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.008344\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.001305\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.000169\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.209091\n",
      "\n",
      "Train set: Average loss: 0.0010, Accuracy: 59676/60000 (99.5%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1046, Accuracy: 9773/10000 (97.730%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.032613\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.003678\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.002946\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.000543\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.000583\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.000173\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.076135\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.068522\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.023172\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.093450\n",
      "\n",
      "Train set: Average loss: 0.0082, Accuracy: 59718/60000 (99.5%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1093, Accuracy: 9767/10000 (97.670%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.000738\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.001211\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.014425\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.007711\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.005435\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.037593\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.003803\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.047950\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.004568\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.000651\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 59739/60000 (99.6%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1160, Accuracy: 9769/10000 (97.690%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.021374\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.004029\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.002165\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.011178\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.083382\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.000737\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.001611\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.009601\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.037307\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.001926\n",
      "\n",
      "Train set: Average loss: 0.0001, Accuracy: 59790/60000 (99.7%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1302, Accuracy: 9742/10000 (97.420%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.020793\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.020461\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.000825\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.000342\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.054267\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.029426\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.013000\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.006176\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.025901\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.094598\n",
      "\n",
      "Train set: Average loss: 0.0002, Accuracy: 59785/60000 (99.6%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1185, Accuracy: 9772/10000 (97.720%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.005143\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.058935\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.006847\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.000381\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.000896\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.041949\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.000888\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.000433\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.000797\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.037117\n",
      "\n",
      "Train set: Average loss: 0.0002, Accuracy: 59816/60000 (99.7%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1308, Accuracy: 9749/10000 (97.490%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.001368\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.017576\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.001113\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.000788\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.004898\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.004225\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.160088\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.003868\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.003143\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.005443\n",
      "\n",
      "Train set: Average loss: 0.0007, Accuracy: 59853/60000 (99.8%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1293, Accuracy: 9763/10000 (97.630%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.004515\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.015755\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.002151\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.000451\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.000401\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.001280\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.000051\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.005602\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.005109\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.003313\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 59879/60000 (99.8%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1264, Accuracy: 9782/10000 (97.820%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# if you have a GPU available or you are using google colab with a GPU environment \n",
    "# (torch.cuda.is_available should be True), you can add an additional argument device='cuda'\n",
    "# to both train and test functions, and move mlp to GPU (mlp = mlp.to('cuda'))\n",
    "epochs = 10\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(mlp, dl_train, optimizer, epoch, log=True, log_interval=100)\n",
    "    test(mlp, dl_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, you should see test accuracies of > 96% - this is the performance we saw in our last experiments with EDF. By the way, here we report test accuracy, the last exercises reported test error. Accuracy is simply (1 - error). Both metrics are commonly reported, there is no clear preference in literature for one or the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you move on to the next exercise, you should play with the different parameters (learning rate, epochs, number of hidden layers, a different optimizer, etc.) to get a feeling what can improve or hamper performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01, 5, 0, 32, <class 'torch.optim.adam.Adam'>\n",
      "\n",
      "Test set: Average loss: 0.6494, Accuracy: 7623/10000 (76.230%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.6566, Accuracy: 7589/10000 (75.890%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.6573, Accuracy: 7606/10000 (76.060%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.6640, Accuracy: 7600/10000 (76.000%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.6988, Accuracy: 7431/10000 (74.310%)\n",
      "\n",
      "0.01, 5, 0, 32, <class 'torch.optim.nadam.NAdam'>\n",
      "\n",
      "Test set: Average loss: 0.8676, Accuracy: 6688/10000 (66.880%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.8590, Accuracy: 6657/10000 (66.570%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.8528, Accuracy: 6710/10000 (67.100%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.8682, Accuracy: 6701/10000 (67.010%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.8391, Accuracy: 6702/10000 (67.020%)\n",
      "\n",
      "0.01, 5, 0, 64, <class 'torch.optim.adam.Adam'>\n",
      "\n",
      "Test set: Average loss: 0.4630, Accuracy: 8362/10000 (83.620%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.2468, Accuracy: 9360/10000 (93.600%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.2427, Accuracy: 9350/10000 (93.500%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1999, Accuracy: 9478/10000 (94.780%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.2120, Accuracy: 9489/10000 (94.890%)\n",
      "\n",
      "0.01, 5, 0, 64, <class 'torch.optim.nadam.NAdam'>\n",
      "\n",
      "Test set: Average loss: 0.1964, Accuracy: 9415/10000 (94.150%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1824, Accuracy: 9522/10000 (95.220%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.2349, Accuracy: 9444/10000 (94.440%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.2301, Accuracy: 9428/10000 (94.280%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1862, Accuracy: 9528/10000 (95.280%)\n",
      "\n",
      "0.01, 5, 0, 182, <class 'torch.optim.adam.Adam'>\n",
      "\n",
      "Test set: Average loss: 1.3118, Accuracy: 4733/10000 (47.330%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.0570, Accuracy: 5751/10000 (57.510%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.0766, Accuracy: 5817/10000 (58.170%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.0749, Accuracy: 5718/10000 (57.180%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.1054, Accuracy: 5762/10000 (57.620%)\n",
      "\n",
      "0.01, 5, 0, 182, <class 'torch.optim.nadam.NAdam'>\n",
      "\n",
      "Test set: Average loss: 2.3026, Accuracy: 980/10000 (9.800%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 2.3026, Accuracy: 980/10000 (9.800%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 2.3026, Accuracy: 980/10000 (9.800%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 2.3026, Accuracy: 980/10000 (9.800%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 2.3026, Accuracy: 980/10000 (9.800%)\n",
      "\n",
      "0.01, 5, 1, 32, <class 'torch.optim.adam.Adam'>\n",
      "\n",
      "Test set: Average loss: 0.6624, Accuracy: 7581/10000 (75.810%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.6512, Accuracy: 7595/10000 (75.950%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.6677, Accuracy: 7514/10000 (75.140%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.6594, Accuracy: 7585/10000 (75.850%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.6575, Accuracy: 7578/10000 (75.780%)\n",
      "\n",
      "0.01, 5, 1, 32, <class 'torch.optim.nadam.NAdam'>\n",
      "\n",
      "Test set: Average loss: 1.1218, Accuracy: 6547/10000 (65.470%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.0996, Accuracy: 6522/10000 (65.220%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.0706, Accuracy: 6607/10000 (66.070%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.0722, Accuracy: 6602/10000 (66.020%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.0718, Accuracy: 6655/10000 (66.550%)\n",
      "\n",
      "0.01, 5, 1, 64, <class 'torch.optim.adam.Adam'>\n",
      "\n",
      "Test set: Average loss: 0.9769, Accuracy: 6217/10000 (62.170%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.7040, Accuracy: 7374/10000 (73.740%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.6578, Accuracy: 7445/10000 (74.450%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.6680, Accuracy: 7432/10000 (74.320%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.6675, Accuracy: 7450/10000 (74.500%)\n",
      "\n",
      "0.01, 5, 1, 64, <class 'torch.optim.nadam.NAdam'>\n",
      "\n",
      "Test set: Average loss: 0.4495, Accuracy: 8419/10000 (84.190%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.4316, Accuracy: 8531/10000 (85.310%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.4391, Accuracy: 8557/10000 (85.570%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.4092, Accuracy: 8574/10000 (85.740%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.4349, Accuracy: 8574/10000 (85.740%)\n",
      "\n",
      "0.01, 5, 1, 182, <class 'torch.optim.adam.Adam'>\n",
      "\n",
      "Test set: Average loss: 2.3026, Accuracy: 980/10000 (9.800%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 2.3026, Accuracy: 980/10000 (9.800%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 2.3026, Accuracy: 980/10000 (9.800%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 2.3026, Accuracy: 980/10000 (9.800%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 2.3026, Accuracy: 980/10000 (9.800%)\n",
      "\n",
      "0.01, 5, 1, 182, <class 'torch.optim.nadam.NAdam'>\n",
      "\n",
      "Test set: Average loss: 1.9041, Accuracy: 1914/10000 (19.140%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.8821, Accuracy: 1911/10000 (19.110%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.8923, Accuracy: 1876/10000 (18.760%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.8846, Accuracy: 1896/10000 (18.960%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.8831, Accuracy: 1901/10000 (19.010%)\n",
      "\n",
      "0.01, 5, 2, 32, <class 'torch.optim.adam.Adam'>\n",
      "\n",
      "Test set: Average loss: 0.8457, Accuracy: 7626/10000 (76.260%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.6684, Accuracy: 8386/10000 (83.860%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.6160, Accuracy: 8527/10000 (85.270%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.6379, Accuracy: 8519/10000 (85.190%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.6141, Accuracy: 8598/10000 (85.980%)\n",
      "\n",
      "0.01, 5, 2, 32, <class 'torch.optim.nadam.NAdam'>\n",
      "\n",
      "Test set: Average loss: 0.9749, Accuracy: 6273/10000 (62.730%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.9178, Accuracy: 6450/10000 (64.500%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.8973, Accuracy: 6506/10000 (65.060%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.7070, Accuracy: 7407/10000 (74.070%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.6922, Accuracy: 7408/10000 (74.080%)\n",
      "\n",
      "0.01, 5, 2, 64, <class 'torch.optim.adam.Adam'>\n",
      "\n",
      "Test set: Average loss: 0.8924, Accuracy: 7449/10000 (74.490%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.8569, Accuracy: 7593/10000 (75.930%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.7397, Accuracy: 8404/10000 (84.040%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.6385, Accuracy: 8543/10000 (85.430%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.6364, Accuracy: 8526/10000 (85.260%)\n",
      "\n",
      "0.01, 5, 2, 64, <class 'torch.optim.nadam.NAdam'>\n",
      "\n",
      "Test set: Average loss: 2.3026, Accuracy: 980/10000 (9.800%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 2.3026, Accuracy: 980/10000 (9.800%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 2.3026, Accuracy: 980/10000 (9.800%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 2.3026, Accuracy: 980/10000 (9.800%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 2.3026, Accuracy: 980/10000 (9.800%)\n",
      "\n",
      "0.01, 5, 2, 182, <class 'torch.optim.adam.Adam'>\n",
      "\n",
      "Test set: Average loss: 1.8525, Accuracy: 3038/10000 (30.380%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.8396, Accuracy: 3083/10000 (30.830%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.8463, Accuracy: 3053/10000 (30.530%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.8449, Accuracy: 3066/10000 (30.660%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.8432, Accuracy: 3064/10000 (30.640%)\n",
      "\n",
      "0.01, 5, 2, 182, <class 'torch.optim.nadam.NAdam'>\n",
      "\n",
      "Test set: Average loss: 2.3026, Accuracy: 980/10000 (9.800%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 2.3026, Accuracy: 980/10000 (9.800%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 2.3026, Accuracy: 980/10000 (9.800%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 2.3026, Accuracy: 980/10000 (9.800%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 2.3026, Accuracy: 980/10000 (9.800%)\n",
      "\n",
      "0.01, 10, 0, 32, <class 'torch.optim.adam.Adam'>\n",
      "\n",
      "Test set: Average loss: 1.0768, Accuracy: 5791/10000 (57.910%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.9254, Accuracy: 6494/10000 (64.940%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.8929, Accuracy: 6621/10000 (66.210%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.8733, Accuracy: 6634/10000 (66.340%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.8673, Accuracy: 6618/10000 (66.180%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.8810, Accuracy: 6587/10000 (65.870%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.9305, Accuracy: 6632/10000 (66.320%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.9038, Accuracy: 6525/10000 (65.250%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.8880, Accuracy: 6635/10000 (66.350%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.9148, Accuracy: 6590/10000 (65.900%)\n",
      "\n",
      "0.01, 10, 0, 32, <class 'torch.optim.nadam.NAdam'>\n",
      "\n",
      "Test set: Average loss: 0.2616, Accuracy: 9256/10000 (92.560%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.2145, Accuracy: 9372/10000 (93.720%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.2103, Accuracy: 9422/10000 (94.220%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.2092, Accuracy: 9431/10000 (94.310%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.2412, Accuracy: 9333/10000 (93.330%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.2434, Accuracy: 9345/10000 (93.450%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.2073, Accuracy: 9460/10000 (94.600%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.2184, Accuracy: 9481/10000 (94.810%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.2512, Accuracy: 9399/10000 (93.990%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.2947, Accuracy: 9283/10000 (92.830%)\n",
      "\n",
      "0.01, 10, 0, 64, <class 'torch.optim.adam.Adam'>\n",
      "\n",
      "Test set: Average loss: 0.8441, Accuracy: 6743/10000 (67.430%)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [44], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[39mfor\u001b[39;00m o \u001b[39min\u001b[39;00m optims:\n\u001b[1;32m     12\u001b[0m                     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00ml\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mh\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00ms\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mo\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m                     test_params(l, e, h, s, o)\n\u001b[1;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtest_params\u001b[39m(l, e, h, s, o):\n\u001b[1;32m     18\u001b[0m     mlp \u001b[39m=\u001b[39m MLP(nInput, nOutput, h, s, act_fn)\n",
      "Cell \u001b[0;32mIn [36], line 19\u001b[0m, in \u001b[0;36mtest_params\u001b[0;34m(l, e, h, s, o)\u001b[0m\n\u001b[1;32m     17\u001b[0m optimizer \u001b[39m=\u001b[39m o(mlp\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39ml)\n\u001b[1;32m     18\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, e \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m---> 19\u001b[0m     train(mlp, dl_train, optimizer, epoch, log\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     20\u001b[0m     test(mlp, dl_test)\n",
      "Cell \u001b[0;32mIn [42], line 4\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dl_train, optimizer, epoch, log, log_interval, device)\u001b[0m\n\u001b[1;32m      2\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m      3\u001b[0m correct \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, (data, target) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dl_train):\n\u001b[1;32m      5\u001b[0m     data, target \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto(device), target\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m     \u001b[39m# first we need to zero the gradient, otherwise PyTorch would accumulate them\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/deep_learning_ex_1/lib/python3.10/site-packages/torch/utils/data/dataloader.py:635\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    633\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    634\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 635\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    636\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    637\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    639\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/deep_learning_ex_1/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1330\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1327\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[1;32m   1329\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1330\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[1;32m   1331\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1332\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[1;32m   1333\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/deep_learning_ex_1/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1296\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1292\u001b[0m     \u001b[39m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m     \u001b[39m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1294\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1295\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m-> 1296\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[1;32m   1297\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[1;32m   1298\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/deep_learning_ex_1/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1134\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1121\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_try_get_data\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m_utils\u001b[39m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1122\u001b[0m     \u001b[39m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m     \u001b[39m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1131\u001b[0m     \u001b[39m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m     \u001b[39m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1134\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m   1135\u001b[0m         \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n\u001b[1;32m   1136\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1137\u001b[0m         \u001b[39m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m         \u001b[39m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m         \u001b[39m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/deep_learning_ex_1/lib/python3.10/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[39m=\u001b[39m deadline \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout):\n\u001b[1;32m    114\u001b[0m         \u001b[39mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_poll():\n",
      "File \u001b[0;32m~/miniconda3/envs/deep_learning_ex_1/lib/python3.10/multiprocessing/connection.py:262\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_closed()\n\u001b[1;32m    261\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_readable()\n\u001b[0;32m--> 262\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout)\n",
      "File \u001b[0;32m~/miniconda3/envs/deep_learning_ex_1/lib/python3.10/multiprocessing/connection.py:429\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_poll\u001b[39m(\u001b[39mself\u001b[39m, timeout):\n\u001b[0;32m--> 429\u001b[0m     r \u001b[39m=\u001b[39m wait([\u001b[39mself\u001b[39;49m], timeout)\n\u001b[1;32m    430\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mbool\u001b[39m(r)\n",
      "File \u001b[0;32m~/miniconda3/envs/deep_learning_ex_1/lib/python3.10/multiprocessing/connection.py:936\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    933\u001b[0m     deadline \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mmonotonic() \u001b[39m+\u001b[39m timeout\n\u001b[1;32m    935\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 936\u001b[0m     ready \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39;49mselect(timeout)\n\u001b[1;32m    937\u001b[0m     \u001b[39mif\u001b[39;00m ready:\n\u001b[1;32m    938\u001b[0m         \u001b[39mreturn\u001b[39;00m [key\u001b[39m.\u001b[39mfileobj \u001b[39mfor\u001b[39;00m (key, events) \u001b[39min\u001b[39;00m ready]\n",
      "File \u001b[0;32m~/miniconda3/envs/deep_learning_ex_1/lib/python3.10/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[39m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mpoll(timeout)\n\u001b[1;32m    417\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[39mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lrs = [0.01, 0.1, 0.5]\n",
    "eps = [5, 10]\n",
    "hlyrs = [0, 1, 2]\n",
    "sizes = [32, 64, 182]\n",
    "optims = [optim.Adam, optim.NAdam]\n",
    "\n",
    "for l in lrs:\n",
    "    for e in eps:\n",
    "        for h in hlyrs:\n",
    "            for s in sizes:\n",
    "                for o in optims:\n",
    "                    print(f\"{l}, {e}, {h}, {s}, {o}\")\n",
    "                    test_params(l, e, h, s, o)\n",
    "\n",
    "\n",
    "def test_params(l, e, h, s, o):\n",
    "\n",
    "    mlp = MLP(nInput, nOutput, h, s, act_fn)\n",
    "    optimizer = o(mlp.parameters(), lr=l)\n",
    "    \n",
    "    for epoch in range(1, e + 1):\n",
    "        train(mlp, dl_train, optimizer, epoch, log=False)\n",
    "    \n",
    "    test(mlp, dl_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN\n",
    "Alright, we matched our prior performance. Let's surpass it!\n",
    "We will build a small CNN. The structure should be as follows\n",
    "\n",
    "| CNN Architecture                             \t|\n",
    "|----------------------------------------------\t|\n",
    "| Conv: $C_{in}=1, C_{out}=32, K=3, S=1, P=0$  \t|\n",
    "| ReLU                                         \t|\n",
    "| Conv: $C_{in}=32, C_{out}=64, K=3, S=1, P=0$ \t|\n",
    "| ReLU                                         \t|\n",
    "| MaxPool2d: $K=2, S=2, P=0$                   \t|\n",
    "| Dropout: $p=0.25$                            \t|\n",
    "| Linear: $C_{in}=9216, C_{out}=128$           \t|\n",
    "| ReLU                                         \t|\n",
    "| Dropout: $p=0.5$                             \t|\n",
    "| Linear: $C_{in}=128, C_{out}=10$             \t|\n",
    "| Log softmax (see above in MLP definition)    \t|\n",
    "\n",
    "The layers you will need are: \n",
    "\n",
    "`nn.Conv2d,  nn.Linear, nn.ReLU,  nn.Dropout, nn.MaxPool2d, nn.LogSoftmax, nn.Flatten`\n",
    "\n",
    "For layers with out parameters you can alternatively use function in the forward pass:  \n",
    "\n",
    "`F.relu, F.max_pool2d, torch.flatten, F.log_softmax`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrinterModule(nn.Module):\n",
    "    def forward(x):\n",
    "        print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "# TODO: Implement the __init__ and forward method of the CNN class. \n",
    "# Hint: do not forget to flatten the appropriate dimension after the convolutional blocks. \n",
    "# A linear layers expect input of the size (B, H) with batch size B and feature size H\n",
    "#########################################################################\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        layers.append(nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=0))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=0))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.MaxPool2d(kernel_size=2, stride=2, padding=0))\n",
    "        layers.append(nn.Dropout(p=0.25))\n",
    "        layers.append(nn.Flatten())\n",
    "        layers.append(nn.Linear(in_features=9216, out_features=128))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Dropout(p=0.5))\n",
    "        layers.append(nn.Linear(in_features=128, out_features=10))\n",
    "        layers.append(nn.LogSoftmax(dim=1))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's test if the forward pass works\n",
    "# this should print torch.Size([1, 10])\n",
    "t = torch.randn(1,1,28,28)\n",
    "cnn = CNN()\n",
    "cnn(t).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, let's train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1.0\n",
    "optimizer = optim.Adadelta(cnn.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.014645\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.005518\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.015431\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.009321\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.066880\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.001795\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.079328\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.022146\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.052973\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.013047\n",
      "\n",
      "Train set: Average loss: 0.0022, Accuracy: 59288/60000 (98.8%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0331, Accuracy: 9911/10000 (99.110%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.003234\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.031607\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.000219\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.005963\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.005711\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.002956\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.073560\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.006247\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.005392\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.022252\n",
      "\n",
      "Train set: Average loss: 0.1460, Accuracy: 59381/60000 (99.0%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0284, Accuracy: 9907/10000 (99.070%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.008225\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.002290\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.051655\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.098430\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.099390\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.059000\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.002535\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.017912\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.065006\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.020879\n",
      "\n",
      "Train set: Average loss: 0.0089, Accuracy: 59417/60000 (99.0%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0318, Accuracy: 9904/10000 (99.040%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.005334\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.085081\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.103222\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.000217\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.007253\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.035430\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.000866\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.031067\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.005604\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.056305\n",
      "\n",
      "Train set: Average loss: 0.0157, Accuracy: 59425/60000 (99.0%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0362, Accuracy: 9911/10000 (99.110%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.040085\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.050220\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.007095\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.000937\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.050446\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.000768\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.006445\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.055341\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.000180\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.111547\n",
      "\n",
      "Train set: Average loss: 0.0047, Accuracy: 59430/60000 (99.0%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0336, Accuracy: 9915/10000 (99.150%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.004233\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.061005\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.002127\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.018609\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.000471\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.001702\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.024921\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.069783\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.001001\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.011443\n",
      "\n",
      "Train set: Average loss: 0.0054, Accuracy: 59513/60000 (99.2%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0321, Accuracy: 9920/10000 (99.200%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.054873\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.005777\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.031759\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.008565\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.023102\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.016468\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.005261\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.009043\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.007477\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.010611\n",
      "\n",
      "Train set: Average loss: 0.0171, Accuracy: 59485/60000 (99.1%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0327, Accuracy: 9918/10000 (99.180%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.010372\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.001702\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.004646\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.001578\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.001266\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.032552\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.093007\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.001233\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.002627\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.000617\n",
      "\n",
      "Train set: Average loss: 0.0585, Accuracy: 59576/60000 (99.3%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0384, Accuracy: 9918/10000 (99.180%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.035675\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.102142\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.000015\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.002602\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.000679\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.032315\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.063230\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.031380\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.007037\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.019670\n",
      "\n",
      "Train set: Average loss: 0.0021, Accuracy: 59594/60000 (99.3%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0326, Accuracy: 9907/10000 (99.070%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.027559\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.003748\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.006456\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.007313\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.007905\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.013313\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.044471\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.000188\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.010084\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.041283\n",
      "\n",
      "Train set: Average loss: 0.0010, Accuracy: 59629/60000 (99.4%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0433, Accuracy: 9919/10000 (99.190%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# if you have a GPU available or you are using google colab with a GPU environment \n",
    "# (torch.cuda.is_available should be True), you can add an additional argument device='cuda'\n",
    "# to both train and test functions, and move cnn to GPU (cnn = cnn.to('cuda'))\n",
    "epochs = 10\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(cnn, dl_train, optimizer, epoch, log_interval=100)\n",
    "    test(cnn, dl_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will probably take a bit longer to train, as a convolutional network is not very efficient on a CPU. The current settings should get you around 99% accuracy. Nice! \n",
    "Again, you should try different hyperparameters and see how far you can push the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on CIFAR10\n",
    "\n",
    "Now we are going to move to something more challenging - CIFAR10. We can reuse most of the code above. Thankfully, CIFAR is also a popular dataset, so we can again make use of a PyTorch convience function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
     ]
    }
   ],
   "source": [
    "ds_train = datasets.CIFAR10(root='./data', train=True, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is not normalized yet, so we need to calculate the normalization constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ims_train = torch.tensor(ds_train.data)\n",
    "ims_train = ims_train.float() / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "# TODO: calculate the mean and std of CIFAR\n",
    "# hint: We want the mean and std of the channel dimension, these should\n",
    "# be 3 dimensional\n",
    "#########################################################################\n",
    "mu = 0.0\n",
    "std = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For CIFAR we want to make use of data augmentation to improve generalization. You will find all data augmentations that are included in torchvision here:\n",
    "\n",
    "https://pytorch.org/docs/stable/torchvision/transforms.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "# TODO: Implement the proper transforms for the training and test dataloaders. \n",
    "# Then build train and test dataloaders with batch size 128 and 2 workers\n",
    "#\n",
    "# Train: \n",
    "# - Apply a random crop with sz 32 on a padded version of the image with P=4\n",
    "# - Flip the image horizontally with a probability of 40 %\n",
    "# - Transform to a Tensor\n",
    "# - Normalize with the constants calculated above\n",
    "# Test: \n",
    "# - Transform to a Tensor\n",
    "# - Normalize with the constants calculated above\n",
    "#########################################################################\n",
    "transform_train = None\n",
    "transform_test = None\n",
    "\n",
    "ds_train = datasets.CIFAR10('./data', train=True, download=True, transform=transform_train)\n",
    "ds_test = datasets.CIFAR10('./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "dl_train = None\n",
    "dl_test = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the  optimizer, this time we use SGD. The scheduler adapts the learning rate during traing (you can ignore it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNN()\n",
    "optimizer = optim.SGD(cnn.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(cnn, dl_train, optimizer, epoch, log_interval=100)\n",
    "    test(cnn, dl_test)    \n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will not work. You should see the following error message\n",
    "\n",
    "```\n",
    "Given groups=1, weight of size [32, 1, 3, 3], expected input[128, 3, 32, 32] to have 1 channels, but got 3 channels instead\n",
    "```\n",
    "\n",
    "This error is telling us that something is not right in the definition of our model. Copy the CNN class from above and make changes, so the training works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "# TODO: Adapt the definition from above to work on CIFAR.\n",
    "# You can copy and run the following prompt for evaluation:\n",
    "# CNN()(torch.randn(1,3,32,32)).shape\n",
    "# It should print 'torch.Size([1, 10])'\n",
    "# Hint: You need to change 2 things. \n",
    "#########################################################################\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn = CNN()\n",
    "optimizer = optim.SGD(cnn.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(cnn, dl_train, optimizer, epoch, log_interval=100)\n",
    "    test(cnn, dl_test)    \n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should give 40 - 50 % - and if you are not already on Colab it will give you a stressed out laptop. The performance is a lot better than random, but we can definitely do better.\n",
    "\n",
    "If you didn't already, move to colab. To use a GPU, follow on the collaboratory menu tabs, \"Runtime\" => \"Change runtime type\" and set it to GPU. Then run the same training loop but now on GPU. \n",
    "\n",
    "It as easy as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "if device == 'cuda': torch.backend.cudnn.benchmark = True # additional speed up\n",
    "\n",
    "cnn = CNN()\n",
    "optimizer = optim.SGD(cnn.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "cnn = cnn.to(device)\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(cnn, dl_train, optimizer, epoch, log_interval=100, device=device)\n",
    "    test(cnn, dl_test, device=device)    \n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should be way faster now. But the true advantage of the GPU is that we can use much bigger models now and still train them in a reasonable amount of time. PyTorch is again very handy. The torchvision library comes with varies state-of-the-art model architectures, some of which you have seen in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn = resnet18()\n",
    "cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks scary! But the only thing you need to change to make it work on CIFAR is the last layer.\n",
    "Currently the last layer is:\n",
    "```\n",
    "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
    "```\n",
    "out_features is the number of classes. This models are developed for Imagenet, a dataset with 1000 classes. So this part of the model you need to adapt. Additionally, you need to add a log-softmax layer again, as we us negative log-likelihood as the training criterion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "# TODO: Adapt the Resnet to work on CIFAR\n",
    "#########################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should print 'torch.Size([16, 10])'\n",
    "cnn(torch.randn(16,3,32,32)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "if device == 'cuda': torch.backends.cudnn.benchmark = True # this gives us additional speed up\n",
    "\n",
    "optimizer = optim.SGD(cnn.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "cnn = cnn.to(device)\n",
    "\n",
    "epochs = 50\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(cnn, dl_train, optimizer, epoch, log_interval=100, device=device)\n",
    "    test(cnn, dl_test, device=device)    \n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should get us well above 75%, the best we got was ~ 80%.\n",
    "\n",
    "Bonus Task: Use different torchvision architectures, different optimizers (Adam is always a good choice), data augmentation techniques, and hyperparameter search to achieve a test accuracy of >90 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning_ex_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "fa132b8639846cf32864305b3f5300c740d597a6683ebe7fed42aa341ffae690"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
